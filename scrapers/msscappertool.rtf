{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ```python\
import asyncio\
import aiohttp\
from bs4 import BeautifulSoup\
import pandas as pd\
from urllib.parse import urljoin\
import re\
import logging\
from typing import Dict, List\
import platform\
\
# Configure logging\
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\
logger = logging.getLogger(__name__)\
\
class MobileSentrixScraper:\
    def __init__(self):\
        self.base_url = "https://www.mobilesentrix.com/"\
        self.headers = \{\
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",\
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",\
            "Accept-Language": "en-US,en;q=0.5",\
        \}\
        self.products_data = []\
        self.max_pages_per_category = 10  # Adjust based on needs\
        self.rate_limit_delay = 1.0  # Seconds between requests\
\
    async def fetch_page(self, session: aiohttp.ClientSession, url: str) -> str:\
        """Fetch page content with rate limiting."""\
        try:\
            async with session.get(url, headers=self.headers) as response:\
                if response.status == 200:\
                    return await response.text()\
                else:\
                    logger.error(f"Failed to fetch \{url\}: Status \{response.status\}")\
                    return None\
        except Exception as e:\
            logger.error(f"Error fetching \{url\}: \{e\}")\
            return None\
\
    async def get_category_links(self, session: aiohttp.ClientSession) -> List[str]:\
        """Extract category links from the homepage."""\
        homepage = await self.fetch_page(session, self.base_url)\
        if not homepage:\
            return []\
\
        soup = BeautifulSoup(homepage, 'html.parser')\
        category_links = []\
        # Adjust selector based on MobileSentrix's navigation structure\
        nav_menu = soup.select("ul.nav-menu li.nav-item a")\
        for link in nav_menu:\
            href = link.get('href', '')\
            if href and not href.startswith('#') and 'category' in href.lower():\
                full_url = urljoin(self.base_url, href)\
                category_links.append(full_url)\
\
        logger.info(f"Found \{len(category_links)\} category links")\
        return list(set(category_links))  # Remove duplicates\
\
    async def scrape_category(self, session: aiohttp.ClientSession, category_url: str):\
        """Scrape products from a category across its pages."""\
        page = 1\
        while page <= self.max_pages_per_category:\
            # MobileSentrix uses ?page= for pagination\
            page_url = f"\{category_url\}?page=\{page\}" if page > 1 else category_url\
            logger.info(f"Scraping category page: \{page_url\}")\
            html = await self.fetch_page(session, page_url)\
            if not html:\
                break\
\
            soup = BeautifulSoup(html, 'html.parser')\
            products = soup.select("div.product-card")  # Adjust selector for product grid\
            if not products:\
                logger.info(f"No more products found in \{category_url\} at page \{page\}")\
                break\
\
            for product in products:\
                try:\
                    product_data = await self.parse_product(product, session)\
                    if product_data:\
                        self.products_data.append(product_data)\
                except Exception as e:\
                    logger.error(f"Error parsing product: \{e\}")\
\
            page += 1\
            await asyncio.sleep(self.rate_limit_delay)  # Respect rate limiting\
\
    async def parse_product(self, product: BeautifulSoup, session: aiohttp.ClientSession) -> Dict:\
        """Parse individual product data."""\
        try:\
            # Extract product name\
            name_elem = product.select_one("h2.product-title a") or product.select_one("a.product-link")\
            name = name_elem.get_text(strip=True) if name_elem else "N/A"\
\
            # Extract product URL\
            product_url = urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.get('href') else None\
\
            # Extract price\
            price_elem = product.select_one("span.price--main, span.price")\
            price = price_elem.get_text(strip=True) if price_elem else "N/A"\
            price = re.sub(r'[^\\d.]', '', price) if price != "N/A" else None\
\
            # Extract image\
            img_elem = product.select_one("img.product-img, img.primary-image")\
            img_url = urljoin(self.base_url, img_elem['src']) if img_elem and img_elem.get('src') else None\
\
            # Fetch product page for specs\
            specs = "N/A"\
            if product_url:\
                product_html = await self.fetch_page(session, product_url)\
                if product_html:\
                    product_soup = BeautifulSoup(product_html, 'html.parser')\
                    specs_elem = product_soup.select_one("div.product-description, div.specs-table")\
                    specs = specs_elem.get_text(strip=True) if specs_elem else "N/A"\
\
            return \{\
                "name": name,\
                "price": float(price) if price else None,\
                "image_url": img_url,\
                "specifications": specs,\
                "product_url": product_url\
            \}\
        except Exception as e:\
            logger.error(f"Error parsing product: \{e\}")\
            return None\
\
    async def save_to_csv(self, filename: str = "mobilesentrix_products.csv"):\
        """Save scraped data to CSV."""\
        if not self.products_data:\
            logger.warning("No data to save")\
            return\
\
        df = pd.DataFrame(self.products_data)\
        df.to_csv(filename, index=False, encoding='utf-8')\
        logger.info(f"Saved \{len(self.products_data)\} products to \{filename\}")\
\
    async def run(self):\
        """Main scraping loop."""\
        async with aiohttp.ClientSession() as session:\
            category_links = await self.get_category_links(session)\
            if not category_links:\
                logger.error("No category links found. Exiting.")\
                return\
\
            for category_url in category_links:\
                await self.scrape_category(session, category_url)\
                await asyncio.sleep(self.rate_limit_delay)  # Delay between categories\
\
            await self.save_to_csv()\
\
async def main():\
    scraper = MobileSentrixScraper()\
    await scraper.run()\
\
if platform.system() == "Emscripten":\
    asyncio.ensure_future(main())\
else:\
    if __name__ == "__main__":\
        asyncio.run(main())\
```}